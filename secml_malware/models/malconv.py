"""
Malware Detection by Eating a Whole EXE
Edward Raff, Jon Barker, Jared Sylvester, Robert Brandon, Bryan Catanzaro, Charles Nicholas
https://arxiv.org/abs/1710.09435
"""
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import re
import hashlib
import lief
import lightgbm as lgb
import numpy as np
from sklearn.feature_extraction import FeatureHasher

LIEF_MAJOR, LIEF_MINOR, _ = lief.__version__.split(".")
LIEF_EXPORT_OBJECT = int(LIEF_MAJOR) > 0 or (
    int(LIEF_MAJOR) == 0 and int(LIEF_MINOR) >= 10
)

from secml_malware.models.basee2e import End2EndModel

from secml.settings import SECML_PYTORCH_USE_CUDA

use_cuda = torch.cuda.is_available() and SECML_PYTORCH_USE_CUDA


class MalConv(End2EndModel):
	"""
	Architecture implementation.
	"""

	def __init__(self, pretrained_path=None, embedding_size=8, max_input_size=2 ** 20):
		super(MalConv, self).__init__(embedding_size, max_input_size, 256, False)
		self.embedding_1 = nn.Embedding(num_embeddings=257, embedding_dim=embedding_size)
		self.conv1d_1 = nn.Conv1d(in_channels=embedding_size, out_channels=128, kernel_size=(500,), stride=(500,),
								  groups=1, bias=True)
		self.conv1d_2 = nn.Conv1d(in_channels=embedding_size, out_channels=128, kernel_size=(500,), stride=(500,),
								  groups=1, bias=True)
		self.dense_1 = nn.Linear(in_features=128, out_features=128, bias=True)
		self.dense_2 = nn.Linear(in_features=128, out_features=1, bias=True)
		if pretrained_path is not None:
			self.load_simplified_model(pretrained_path)
		if use_cuda:
			self.cuda()

	def embed(self, input_x, transpose=True):
		if isinstance(input_x, torch.Tensor):
			x = input_x.clone().detach().requires_grad_(True).type(torch.LongTensor)
		else:
			x = torch.from_numpy(input_x).type(torch.LongTensor)
		x = x.squeeze(dim=1)
		if use_cuda:
			x = x.cuda()
		emb_x = self.embedding_1(x)
		if transpose:
			emb_x = torch.transpose(emb_x, 1, 2)
		return emb_x

	def embedd_and_forward(self, x):
		conv1d_1 = self.conv1d_1(x)
		conv1d_2 = self.conv1d_2(x)
		conv1d_1_activation = torch.relu(conv1d_1)
		conv1d_2_activation = torch.sigmoid(conv1d_2)
		multiply_1 = conv1d_1_activation * conv1d_2_activation
		global_max_pooling1d_1 = F.max_pool1d(input=multiply_1, kernel_size=multiply_1.size()[2:])
		global_max_pooling1d_1_flatten = global_max_pooling1d_1.view(global_max_pooling1d_1.size(0), -1)
		dense_1 = self.dense_1(global_max_pooling1d_1_flatten)
		dense_1_activation = torch.relu(dense_1)
		dense_2 = self.dense_2(dense_1_activation)
		dense_2_activation = torch.sigmoid(dense_2)
		return dense_2_activation


class AvastNet(End2EndModel):
	# """
	# Architecture implementation.
	# """
	#
	# def __init__(self, pretrained_path=None, embedding_size=16, max_input_size=102400):
	# 	super(AvastNet, self).__init__(embedding_size, max_input_size, 256, False)
	#
	# 	self.embed_1 = nn.Embedding(257, 8)
	# 	self.conv1d_1 = nn.Conv1d(8, 48, 32, 4, bias=True)
	# 	self.conv1d_2 = nn.Conv1d(48, 96, 32, 4, bias=True)
	# 	self.conv1d_3 = nn.Conv1d(96, 128, 16, 8, bias=True)
	# 	self.conv1d_4 = nn.Conv1d(128, 192, 16, 8, bias=True)
	#
	# 	self.avgpool = nn.AdaptiveAvgPool1d(1)
	#
	# 	self.conv_dropout = nn.Dropout(0.2)
	# 	self.dense_dropout = nn.Dropout(0.5)
	#
	# 	self.dense_1 = nn.Linear(192, 160)
	# 	self.dense_2 = nn.Linear(160, 128)
	# 	self.dense_3 = nn.Linear(128, 1)
	#
	# 	if pretrained_path is not None:
	# 		self.load_simplified_model(pretrained_path)
	# 	if use_cuda:
	# 		self.cuda()
	#
	# def embed(self, input_x, transpose=True):
	# 	if isinstance(input_x, torch.Tensor):
	# 		x = input_x.float().clone().detach().requires_grad_(True).type(torch.LongTensor)
	# 	else:
	# 		x = torch.from_numpy(input_x).type(torch.LongTensor)
	# 	x = x.squeeze(dim=1)
	# 	if use_cuda:
	# 		x = x.cuda()
	# 	emb_x = self.embed_1(x)
	# 	if transpose:
	# 		emb_x = torch.transpose(emb_x, -1, -2)
	# 	return emb_x
	#
	# def embedd_and_forward(self, x):
	#
	# 	# x = self.conv_dropout(x)
	#
	# 	x = F.relu(self.conv1d_1(x))
	# 	# x = self.conv_dropout(x)
	#
	# 	x = F.relu(self.conv1d_2(x))
	# 	# x = self.conv_dropout(x)
	#
	# 	x = F.max_pool1d(x, 4, stride=4, padding=0)
	#
	# 	x = F.relu(self.conv1d_3(x))
	# 	# x = self.conv_dropout(x)
	# 	x = F.relu(self.conv1d_4(x))
	# 	# x = self.conv_dropout(x)
	#
	# 	# x = x.view(-1, 192)
	# 	x = self.avgpool(x)
	# 	x = x.squeeze(-1)
	#
	# 	x = F.selu(self.dense_1(x))
	# 	x = F.selu(self.dense_2(x))
	# 	x = F.selu(self.dense_3(x))
	#
	# 	x = torch.sigmoid(x)
	# 	return x
	"""
	Architecture implementation.
	"""

	def __init__(self, pretrained_path=None, embedding_size=16, max_input_size=4096):
		super(AvastNet, self).__init__(embedding_size, max_input_size, 256, False)

		self.embed_1 = nn.Embedding(257, 16)
		self.conv1d_1 = nn.Conv1d(16, 48, 32, 4, padding=14)
		self.conv1d_2 = nn.Conv1d(48, 96, 32, 4, padding=14)
		self.conv1d_3 = nn.Conv1d(96, 128, 16, 8, padding=4)
		self.conv1d_4 = nn.Conv1d(128, 192, 16, 8, padding=4)

		self.conv_dropout = nn.Dropout(0.2)
		self.dense_dropout = nn.Dropout(0.5)

		self.dense_1 = nn.Linear(192*8*8, 256*4)
		self.dense_1_1 = nn.Linear(256*4,64)
		self.dense_2 = nn.Linear(64, 1)

		if pretrained_path is not None:
			self.load_simplified_model(pretrained_path)
		if use_cuda:
			self.cuda()

	def embed(self, input_x, transpose=True):
		if isinstance(input_x, torch.Tensor):
			x = input_x.float().clone().detach().requires_grad_(True).type(torch.LongTensor)
		else:
			x = torch.from_numpy(input_x).type(torch.LongTensor)
		x = x.squeeze(dim=1)
		if use_cuda:
			x = x.cuda()
		emb_x = self.embed_1(x)
		if transpose:
			emb_x = torch.transpose(emb_x, -1, -2)
		return emb_x

	def embedd_and_forward(self, x):

		x = self.conv_dropout(x)

		x = F.relu(self.conv1d_1(x))
		x = self.conv_dropout(x)

		x = F.relu(self.conv1d_2(x))
		x = self.conv_dropout(x)

		x = F.max_pool1d(x, 4, stride=4, padding=0)

		x = F.relu(self.conv1d_3(x))
		x = self.conv_dropout(x)
		x = F.relu(self.conv1d_4(x))
		x = self.conv_dropout(x)

		x = x.view(-1, 192*8*8)

		x = F.selu(self.dense_1(x))

		x = F.selu(self.dense_1_1(x))

		x = self.dense_2(x)

		x = torch.sigmoid(x)
		return x


class FireEye(End2EndModel):
	"""
	Architecture implementation.
	"""

	def __init__(self, max_input_size=1048576, embedding_size=10, window_size=512, vocab_size=257):
		super(FireEye, self).__init__(embedding_size, max_input_size, 256, False)

		self.padding_char = 256
		self.malicious_threshold = 0.5


		self.embed_1 = nn.Embedding(vocab_size, 10, padding_idx=256)

		self.conv_1 = nn.Conv1d(10, 16, 8, stride=4, bias=True)
		self.conv_2 = nn.Conv1d(16, 32, 16, stride=4, bias=True)
		self.conv_3 = nn.Conv1d(32, 64, 4, stride=2, bias=True)
		self.conv_4 = nn.Conv1d(64, 128, 4, stride=2, bias=True)
		self.conv_5 = nn.Conv1d(128, 128, 4, stride=2, bias=True)

		self.pooling = nn.MaxPool1d(int(max_input_size / window_size))
		# self.pooling = nn.AdaptiveMaxPool1d(1)

		# self.fc_1 = nn.Linear(128, 128)
		self.fc_2 = nn.Linear(128 * 3, 1)

		self.sigmoid = nn.Sigmoid()  # binary classification

	# self.softmax = nn.Softmax()   # multi-classfication

	def embed(self, input_x, transpose=True):
		if isinstance(input_x, torch.Tensor):
			x = input_x.float().clone().detach().requires_grad_(True).type(torch.LongTensor)
		else:
			x = torch.from_numpy(input_x).type(torch.LongTensor)
		x = x.squeeze(dim=1)
		if use_cuda:
			x = x.cuda()
		emb_x = self.embed_1(x)
		if transpose:
			emb_x = torch.transpose(emb_x, -1, -2)
		return emb_x

	def embedd_and_forward(self, x):

		# x = self.embed(x.long())
		# Channel first
		# x = torch.transpose(x, -1, -2)

		x = self.conv_1(x)
		# x = self.pooling(x)

		x = self.conv_2(x)
		# x = self.pooling(x)

		x = self.conv_3(x)
		# x = self.pooling(x)

		x = self.conv_4(x)
		# x = self.pooling(x)

		x = self.conv_5(x)
		x = self.pooling(x)

		x = x.view(x.shape[0], -1)
		x = self.fc_2(x)

		x = torch.sigmoid(x)
		return x


class FeatureType:
    """Base class from which each feature type may inherit"""

    name = ""
    dim = 0

    def __repr__(self):
        return f"{self.name}({self.dim})"

    def raw_features(self, bytez, lief_binary):
        """Generate a JSON-able representation of the file"""
        raise (NotImplementedError)

    def process_raw_features(self, raw_obj):
        """Generate a feature vector from the raw features"""
        raise (NotImplementedError)

    def feature_vector(self, bytez, lief_binary):
        """Directly calculate the feature vector from the sample itself. This should only be implemented differently
        if there are significant speedups to be gained from combining the two functions."""
        return self.process_raw_features(self.raw_features(bytez, lief_binary))


class ByteHistogram(FeatureType):
    """Byte histogram (count + non-normalized) over the entire binary file"""

    name = "histogram"
    dim = 256

    def __init__(self):
        super(FeatureType, self).__init__()

    def raw_features(self, bytez, lief_binary):
        counts = np.bincount(
            np.frombuffer(
                bytez,
                dtype=np.uint8,
            ),
            minlength=256,
        )
        return counts.tolist()

    def process_raw_features(self, raw_obj):
        counts = np.array(raw_obj, dtype=np.float32)
        sum = counts.sum()
        normalized = counts / sum
        return normalized


class ByteEntropyHistogram(FeatureType):
    """2d byte/entropy histogram based loosely on (Saxe and Berlin, 2015).
    This roughly approximates the joint probability of byte value and local entropy.
    See Section 2.1.1 in https://arxiv.org/pdf/1508.03096.pdf for more info.
    """

    name = "byteentropy"
    dim = 256

    def __init__(self, step=1024, window=2048):
        super(FeatureType, self).__init__()
        self.window = window
        self.step = step

    def _entropy_bin_counts(self, block):
        # coarse histogram, 16 bytes per bin
        c = np.bincount(block >> 4, minlength=16)  # 16-bin histogram
        p = c.astype(np.float32) / self.window
        wh = np.where(c)[0]
        H = (
            np.sum(
                -p[wh]
                * np.log2(
                    p[wh],
                ),
            )
            * 2
        )  # * x2 b.c. we reduced information by half: 256 bins (8 bits) to 16 bins (4 bits)

        Hbin = int(H * 2)  # up to 16 bins (max entropy is 8 bits)
        if Hbin == 16:  # handle entropy = 8.0 bits
            Hbin = 15

        return Hbin, c

    def raw_features(self, bytez, lief_binary):
        output = np.zeros((16, 16), dtype=np.int)
        a = np.frombuffer(bytez, dtype=np.uint8)
        if a.shape[0] < self.window:
            Hbin, c = self._entropy_bin_counts(a)
            output[Hbin, :] += c
        else:
            # strided trick from here: http://www.rigtorp.se/2011/01/01/rolling-statistics-numpy.html
            shape = a.shape[:-1] + (a.shape[-1] - self.window + 1, self.window)
            strides = a.strides + (a.strides[-1],)
            blocks = np.lib.stride_tricks.as_strided(
                a,
                shape=shape,
                strides=strides,
            )[:: self.step, :]

            # from the blocks, compute histogram
            for block in blocks:
                Hbin, c = self._entropy_bin_counts(block)
                output[Hbin, :] += c

        return output.flatten().tolist()

    def process_raw_features(self, raw_obj):
        counts = np.array(raw_obj, dtype=np.float32)
        sum = counts.sum()
        normalized = counts / sum
        return normalized


class SectionInfo(FeatureType):
    """Information about section names, sizes and entropy.  Uses hashing trick
    to summarize all this section info into a feature vector.
    """

    name = "section"
    dim = 5 + 50 + 50 + 50 + 50 + 50

    def __init__(self):
        super(FeatureType, self).__init__()

    @staticmethod
    def _properties(s):
        return [str(c).split(".")[-1] for c in s.characteristics_lists]

    def raw_features(self, bytez, lief_binary):
        if lief_binary is None:
            return {"entry": "", "sections": []}

        # properties of entry point, or if invalid, the first executable section
        try:
            entry_section = lief_binary.section_from_offset(
                lief_binary.entrypoint,
            ).name
        except lief.not_found:
            # bad entry point, let's find the first executable section
            entry_section = ""
            for s in lief_binary.sections:
                if (
                    lief.PE.SECTION_CHARACTERISTICS.MEM_EXECUTE
                    in s.characteristics_lists
                ):
                    entry_section = s.name
                    break

        raw_obj = {"entry": entry_section}
        raw_obj["sections"] = [
            {
                "name": s.name,
                "size": s.size,
                "entropy": s.entropy,
                "vsize": s.virtual_size,
                "props": self._properties(s),
            }
            for s in lief_binary.sections
        ]
        return raw_obj

    def process_raw_features(self, raw_obj):
        sections = raw_obj["sections"]
        general = [
            len(sections),  # total number of sections
            # number of sections with nonzero size
            sum(1 for s in sections if s["size"] == 0),
            # number of sections with an empty name
            sum(1 for s in sections if s["name"] == ""),
            # number of RX
            sum(
                1
                for s in sections
                if "MEM_READ" in s["props"] and "MEM_EXECUTE" in s["props"]
            ),
            # number of W
            sum(1 for s in sections if "MEM_WRITE" in s["props"]),
        ]
        # gross characteristics of each section
        section_sizes = [(s["name"], s["size"]) for s in sections]
        section_sizes_hashed = (
            FeatureHasher(50, input_type="pair")
            .transform(
                [
                    section_sizes,
                ],
            )
            .toarray()[0]
        )
        section_entropy = [(s["name"], s["entropy"]) for s in sections]
        section_entropy_hashed = (
            FeatureHasher(50, input_type="pair")
            .transform(
                [
                    section_entropy,
                ],
            )
            .toarray()[0]
        )
        section_vsize = [(s["name"], s["vsize"]) for s in sections]
        section_vsize_hashed = (
            FeatureHasher(50, input_type="pair")
            .transform(
                [
                    section_vsize,
                ],
            )
            .toarray()[0]
        )
        entry_name_hashed = (
            FeatureHasher(50, input_type="string")
            .transform(
                [
                    raw_obj["entry"],
                ],
            )
            .toarray()[0]
        )
        characteristics = [
            p for s in sections for p in s["props"] if s["name"] == raw_obj["entry"]
        ]
        characteristics_hashed = (
            FeatureHasher(50, input_type="string")
            .transform(
                [
                    characteristics,
                ],
            )
            .toarray()[0]
        )

        return np.hstack(
            [
                general,
                section_sizes_hashed,
                section_entropy_hashed,
                section_vsize_hashed,
                entry_name_hashed,
                characteristics_hashed,
            ],
        ).astype(np.float32)


class ImportsInfo(FeatureType):
    """Information about imported libraries and functions from the
    import address table.  Note that the total number of imported
    functions is contained in GeneralFileInfo.
    """

    name = "imports"
    dim = 1280

    def __init__(self):
        super(FeatureType, self).__init__()

    def raw_features(self, bytez, lief_binary):
        imports = {}
        if lief_binary is None:
            return imports

        for lib in lief_binary.imports:
            if lib.name not in imports:
                # libraries can be duplicated in listing, extend instead of overwrite
                imports[lib.name] = []

            # Clipping assumes there are diminishing returns on the discriminatory power of imported functions
            #  beyond the first 10000 characters, and this will help limit the dataset size
            for entry in lib.entries:
                if entry.is_ordinal:
                    imports[lib.name].append("ordinal" + str(entry.ordinal))
                else:
                    imports[lib.name].append(entry.name[:10000])

        return imports

    def process_raw_features(self, raw_obj):
        # unique libraries
        libraries = list({l.lower() for l in raw_obj.keys()})
        libraries_hashed = (
            FeatureHasher(
                256,
                input_type="string",
            )
            .transform([libraries])
            .toarray()[0]
        )

        # A string like "kernel32.dll:CreateFileMappingA" for each imported function
        imports = [
            lib.lower() + ":" + e for lib, elist in raw_obj.items() for e in elist
        ]
        imports_hashed = (
            FeatureHasher(
                1024,
                input_type="string",
            )
            .transform([imports])
            .toarray()[0]
        )

        # Two separate elements: libraries (alone) and fully-qualified names of imported functions
        return np.hstack([libraries_hashed, imports_hashed]).astype(np.float32)


class ExportsInfo(FeatureType):
    """Information about exported functions. Note that the total number of exported
    functions is contained in GeneralFileInfo.
    """

    name = "exports"
    dim = 128

    def __init__(self):
        super(FeatureType, self).__init__()

    def raw_features(self, bytez, lief_binary):
        if lief_binary is None:
            return []

        # Clipping assumes there are diminishing returns on the discriminatory power of exports beyond
        #  the first 10000 characters, and this will help limit the dataset size
        if LIEF_EXPORT_OBJECT:
            # export is an object with .name attribute (0.10.0 and later)
            clipped_exports = [
                export.name[:10000] for export in lief_binary.exported_functions
            ]
        else:
            # export is a string (LIEF 0.9.0 and earlier)
            clipped_exports = [
                export[:10000] for export in lief_binary.exported_functions
            ]

        return clipped_exports

    def process_raw_features(self, raw_obj):
        exports_hashed = (
            FeatureHasher(
                128,
                input_type="string",
            )
            .transform([raw_obj])
            .toarray()[0]
        )
        return exports_hashed.astype(np.float32)


class GeneralFileInfo(FeatureType):
    """General information about the file"""

    name = "general"
    dim = 10

    def __init__(self):
        super(FeatureType, self).__init__()

    def raw_features(self, bytez, lief_binary):
        if lief_binary is None:
            return {
                "size": len(bytez),
                "vsize": 0,
                "has_debug": 0,
                "exports": 0,
                "imports": 0,
                "has_relocations": 0,
                "has_resources": 0,
                "has_signature": 0,
                "has_tls": 0,
                "symbols": 0,
            }

        return {
            "size": len(bytez),
            "vsize": lief_binary.virtual_size,
            "has_debug": int(lief_binary.has_debug),
            "exports": len(lief_binary.exported_functions),
            "imports": len(lief_binary.imported_functions),
            "has_relocations": int(lief_binary.has_relocations),
            "has_resources": int(lief_binary.has_resources),
            "has_signature": int(lief_binary.has_signature),
            "has_tls": int(lief_binary.has_tls),
            "symbols": len(lief_binary.symbols),
        }

    def process_raw_features(self, raw_obj):
        return np.asarray(
            [
                raw_obj["size"],
                raw_obj["vsize"],
                raw_obj["has_debug"],
                raw_obj["exports"],
                raw_obj["imports"],
                raw_obj["has_relocations"],
                raw_obj["has_resources"],
                raw_obj["has_signature"],
                raw_obj["has_tls"],
                raw_obj["symbols"],
            ],
            dtype=np.float32,
        )


class HeaderFileInfo(FeatureType):
    """Machine, architecure, OS, linker and other information extracted from header"""

    name = "header"
    dim = 62

    def __init__(self):
        super(FeatureType, self).__init__()

    def raw_features(self, bytez, lief_binary):
        raw_obj = {}
        raw_obj["coff"] = {
            "timestamp": 0,
            "machine": "",
            "characteristics": [],
        }
        raw_obj["optional"] = {
            "subsystem": "",
            "dll_characteristics": [],
            "magic": "",
            "major_image_version": 0,
            "minor_image_version": 0,
            "major_linker_version": 0,
            "minor_linker_version": 0,
            "major_operating_system_version": 0,
            "minor_operating_system_version": 0,
            "major_subsystem_version": 0,
            "minor_subsystem_version": 0,
            "sizeof_code": 0,
            "sizeof_headers": 0,
            "sizeof_heap_commit": 0,
        }
        if lief_binary is None:
            return raw_obj

        raw_obj["coff"]["timestamp"] = lief_binary.header.time_date_stamps
        raw_obj["coff"]["machine"] = str(lief_binary.header.machine).split(
            ".",
        )[-1]
        raw_obj["coff"]["characteristics"] = [
            str(c).split(
                ".",
            )[-1]
            for c in lief_binary.header.characteristics_list
        ]
        raw_obj["optional"]["subsystem"] = str(
            lief_binary.optional_header.subsystem,
        ).split(".")[-1]
        raw_obj["optional"]["dll_characteristics"] = [
            str(c).split(".")[-1]
            for c in lief_binary.optional_header.dll_characteristics_lists
        ]
        raw_obj["optional"]["magic"] = str(lief_binary.optional_header.magic).split(
            ".",
        )[-1]
        raw_obj["optional"][
            "major_image_version"
        ] = lief_binary.optional_header.major_image_version
        raw_obj["optional"][
            "minor_image_version"
        ] = lief_binary.optional_header.minor_image_version
        raw_obj["optional"][
            "major_linker_version"
        ] = lief_binary.optional_header.major_linker_version
        raw_obj["optional"][
            "minor_linker_version"
        ] = lief_binary.optional_header.minor_linker_version
        raw_obj["optional"][
            "major_operating_system_version"
        ] = lief_binary.optional_header.major_operating_system_version
        raw_obj["optional"][
            "minor_operating_system_version"
        ] = lief_binary.optional_header.minor_operating_system_version
        raw_obj["optional"][
            "major_subsystem_version"
        ] = lief_binary.optional_header.major_subsystem_version
        raw_obj["optional"][
            "minor_subsystem_version"
        ] = lief_binary.optional_header.minor_subsystem_version
        raw_obj["optional"]["sizeof_code"] = lief_binary.optional_header.sizeof_code
        raw_obj["optional"][
            "sizeof_headers"
        ] = lief_binary.optional_header.sizeof_headers
        raw_obj["optional"][
            "sizeof_heap_commit"
        ] = lief_binary.optional_header.sizeof_heap_commit
        return raw_obj

    def process_raw_features(self, raw_obj):
        return np.hstack(
            [
                raw_obj["coff"]["timestamp"],
                FeatureHasher(10, input_type="string")
                .transform(
                    [[raw_obj["coff"]["machine"]]],
                )
                .toarray()[0],
                FeatureHasher(10, input_type="string")
                .transform(
                    [raw_obj["coff"]["characteristics"]],
                )
                .toarray()[0],
                FeatureHasher(10, input_type="string")
                .transform(
                    [[raw_obj["optional"]["subsystem"]]],
                )
                .toarray()[0],
                FeatureHasher(10, input_type="string")
                .transform(
                    [raw_obj["optional"]["dll_characteristics"]],
                )
                .toarray()[0],
                FeatureHasher(10, input_type="string")
                .transform(
                    [[raw_obj["optional"]["magic"]]],
                )
                .toarray()[0],
                raw_obj["optional"]["major_image_version"],
                raw_obj["optional"]["minor_image_version"],
                raw_obj["optional"]["major_linker_version"],
                raw_obj["optional"]["minor_linker_version"],
                raw_obj["optional"]["major_operating_system_version"],
                raw_obj["optional"]["minor_operating_system_version"],
                raw_obj["optional"]["major_subsystem_version"],
                raw_obj["optional"]["minor_subsystem_version"],
                raw_obj["optional"]["sizeof_code"],
                raw_obj["optional"]["sizeof_headers"],
                raw_obj["optional"]["sizeof_heap_commit"],
            ],
        ).astype(np.float32)


class StringExtractor(FeatureType):
    """Extracts strings from raw byte stream"""

    name = "strings"
    dim = 1 + 1 + 1 + 96 + 1 + 1 + 1 + 1 + 1

    def __init__(self):
        super(FeatureType, self).__init__()
        # all consecutive runs of 0x20 - 0x7f that are 5+ characters
        self._allstrings = re.compile(b"[\x20-\x7f]{5,}")
        # occurances of the string 'C:\'.  Not actually extracting the path
        self._paths = re.compile(b"c:\\\\", re.IGNORECASE)
        # occurances of http:// or https://.  Not actually extracting the URLs
        self._urls = re.compile(b"https?://", re.IGNORECASE)
        # occurances of the string prefix HKEY_.  No actually extracting registry names
        self._registry = re.compile(b"HKEY_")
        # crude evidence of an MZ header (dropper?) somewhere in the byte stream
        self._mz = re.compile(b"MZ")

    def raw_features(self, bytez, lief_binary):
        allstrings = self._allstrings.findall(bytez)
        if allstrings:
            # statistics about strings:
            string_lengths = [len(s) for s in allstrings]
            avlength = sum(string_lengths) / len(string_lengths)
            # map printable characters 0x20 - 0x7f to an int array consisting of 0-95, inclusive
            as_shifted_string = [b - ord(b"\x20") for b in b"".join(allstrings)]
            c = np.bincount(as_shifted_string, minlength=96)  # histogram count
            # distribution of characters in printable strings
            csum = c.sum()
            p = c.astype(np.float32) / csum
            wh = np.where(c)[0]
            H = np.sum(-p[wh] * np.log2(p[wh]))  # entropy
        else:
            avlength = 0
            c = np.zeros((96,), dtype=np.float32)
            H = 0
            csum = 0

        return {
            "numstrings": len(allstrings),
            "avlength": avlength,
            "printabledist": c.tolist(),  # store non-normalized histogram
            "printables": int(csum),
            "entropy": float(H),
            "paths": len(self._paths.findall(bytez)),
            "urls": len(self._urls.findall(bytez)),
            "registry": len(self._registry.findall(bytez)),
            "MZ": len(self._mz.findall(bytez)),
        }

    def process_raw_features(self, raw_obj):
        hist_divisor = (
            float(
                raw_obj["printables"],
            )
            if raw_obj["printables"] > 0
            else 1.0
        )
        return np.hstack(
            [
                raw_obj["numstrings"],
                raw_obj["avlength"],
                raw_obj["printables"],
                np.asarray(raw_obj["printabledist"]) / hist_divisor,
                raw_obj["entropy"],
                raw_obj["paths"],
                raw_obj["urls"],
                raw_obj["registry"],
                raw_obj["MZ"],
            ],
        ).astype(np.float32)


class DataDirectories(FeatureType):
    """Extracts size and virtual address of the first 15 data directories"""

    name = "datadirectories"
    dim = 15 * 2

    def __init__(self):
        super(FeatureType, self).__init__()
        self._name_order = [
            "EXPORT_TABLE",
            "IMPORT_TABLE",
            "RESOURCE_TABLE",
            "EXCEPTION_TABLE",
            "CERTIFICATE_TABLE",
            "BASE_RELOCATION_TABLE",
            "DEBUG",
            "ARCHITECTURE",
            "GLOBAL_PTR",
            "TLS_TABLE",
            "LOAD_CONFIG_TABLE",
            "BOUND_IMPORT",
            "IAT",
            "DELAY_IMPORT_DESCRIPTOR",
            "CLR_RUNTIME_HEADER",
        ]

    def raw_features(self, bytez, lief_binary):
        output = []
        if lief_binary is None:
            return output

        for data_directory in lief_binary.data_directories:
            output.append(
                {
                    "name": str(data_directory.type).replace("DATA_DIRECTORY.", ""),
                    "size": data_directory.size,
                    "virtual_address": data_directory.rva,
                },
            )
        return output

    def process_raw_features(self, raw_obj):
        features = np.zeros(2 * len(self._name_order), dtype=np.float32)
        for i in range(len(self._name_order)):
            if i < len(raw_obj):
                features[2 * i] = raw_obj[i]["size"]
                features[2 * i + 1] = raw_obj[i]["virtual_address"]
        return features

class PEFeatureExtractor:
    """Extract useful features from a PE file, and return as a vector of fixed size."""

    def __init__(self, feature_version=2):
        self.features = [
            ByteHistogram(),
            ByteEntropyHistogram(),
            StringExtractor(),
            GeneralFileInfo(),
            HeaderFileInfo(),
            SectionInfo(),
            ImportsInfo(),
            ExportsInfo(),
        ]
        if feature_version == 1:
            if not lief.__version__.startswith("0.8.3"):
                print(
                    f"WARNING: EMBER feature version 1 were computed using lief version 0.8.3-18d5b75",
                )
                print(
                    f"WARNING:   lief version {lief.__version__} found instead. There may be slight inconsistencies",
                )
                print(f"WARNING:   in the feature calculations.")
        elif feature_version == 2:
            self.features.append(DataDirectories())
            if not lief.__version__.startswith("0.9.0"):
                print(
                    f"WARNING: EMBER feature version 2 were computed using lief version 0.9.0-",
                )
                print(
                    f"WARNING:   lief version {lief.__version__} found instead. There may be slight inconsistencies",
                )
                print(f"WARNING:   in the feature calculations.")
        else:
            raise Exception(
                f"EMBER feature version must be 1 or 2. Not {feature_version}",
            )
        self.dim = sum(fe.dim for fe in self.features)

    def raw_features(self, bytez):
        lief_errors = (
            lief.bad_format,
            lief.bad_file,
            lief.pe_error,
            lief.parser_error,
            lief.read_out_of_bound,
            RuntimeError,
        )
        try:
            lief_binary = lief.PE.parse(list(bytez))
        except lief_errors as e:
            # print("lief error: ", str(e))
            lief_binary = None
        # everything else (KeyboardInterrupt, SystemExit, ValueError):
        except Exception:
            raise

        features = {"sha256": hashlib.sha256(bytez).hexdigest()}
        features.update(
            {fe.name: fe.raw_features(bytez, lief_binary) for fe in self.features},
        )
        return features

    def process_raw_features(self, raw_obj):
        feature_vectors = [
            fe.process_raw_features(
                raw_obj[fe.name],
            )
            for fe in self.features
        ]
        return np.hstack(feature_vectors).astype(np.float32)

    def feature_vector(self, bytez):
        return self.process_raw_features(self.raw_features(bytez))

class EMBERNN(End2EndModel):
	"""
	Architecture implementation.
	"""

	def __init__(self, max_input_size=1048576):
		super(EMBERNN, self).__init__(8, max_input_size, 256, False)

		self.padding_char = 256
		self.malicious_threshold = 0.5
		self.feature_version = 2

		self.feature_extractor = PEFeatureExtractor(self.feature_version)

		self.dense_1 = nn.Linear(2381, 4000)

		self.dense_2 = nn.Linear(4000, 2000)

		self.dense_3 = nn.Linear(2000, 100)

		self.bn_1 = nn.BatchNorm1d(4000)

		self.bn_2 = nn.BatchNorm1d(2000)

		self.bn_3 = nn.BatchNorm1d(100)

		self.fc_2 = nn.Linear(100, 1)

		self.sigmoid = nn.Sigmoid()  # binary classification

	# self.softmax = nn.Softmax()   # multi-classfication

	def embed(self, input_x, transpose=False):

		ember_feature = []

		for idx in range(input_x.shape[0]):
			sample_slice = np.array(input_x[idx].cpu())
			code = np.delete(sample_slice, np.where(sample_slice == 256))
			x_real = code.tolist()
			x_real_bytes = bytearray(b''.join([bytes([int(i)]) for i in x_real]))
			input_feature = np.array(self.feature_extractor.feature_vector(x_real_bytes), dtype=np.float32)
			ember_feature.append(input_feature)

		ember_feature = np.array(ember_feature)

		if isinstance(ember_feature, torch.Tensor):
			x = ember_feature.float().clone().detach().requires_grad_(True).type(torch.FloatTensor)
		else:
			x = torch.from_numpy(ember_feature).type(torch.FloatTensor)

		x = x.squeeze(dim=1)
		if use_cuda:
			x = x.cuda()

		return x

	def embedd_and_forward(self, x):

		x = F.relu(self.dense_1(x))

		x = self.bn_1(x)

		x = F.dropout(x, 0.5)

		x = F.relu(self.dense_2(x))

		x = self.bn_2(x)

		x = F.dropout(x, 0.5)

		x = F.relu(self.dense_3(x))

		x = self.bn_3(x)

		x = F.dropout(x, 0.5)

		x = x.view(x.shape[0], -1)
		x = self.fc_2(x)

		x = torch.sigmoid(x)

		return x